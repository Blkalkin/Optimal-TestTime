# Reasoning Chain Pruning for Mathematical Problem Solving

This repository contains code for our research on pruning parallel reasoning chains to improve inference time scaling efficiency in mathematical problem solving. This is a theoretical approach to demonstrate how much performance can be improved - with an efficient implementation on the way.

## Research Overview

We investigate a novel approach to efficiently prune redundant reasoning attempts generated by CoT LLMS. Our method:

1. Generates multiple parallel reasoning chains for mathematical problems - we use DeepSeek-R1-Distill-Llama-70B hosted on an 8xH100 for our experiments
3. Extracts final answers using GPT-4o
4. Chunks the reasoning chains into sections 300 tokens long, then embeds them using OpenAI v3 Large
5. Applies clustering over the embeddings to identify and group similar reasoning approaches
6. Keeps reasoning chains that are represented by embeddings at the centroid of the clusters
7. Quantifies theoretical performance scaling using pass@k metrics

## Directory Structure

- `responses/`: Contains JSON files with multiple solution attempts per math problem
- `extracted_answers/`: Contains extracted final answers from each reasoning attempt
- `chunked_embeddings/`: Contains chunked reasoning steps and their vector embeddings for semantic analysis

## Requirements

- Python 3.6+
- OpenAI Python package
- tiktoken (for text chunking)
- tqdm (for progress tracking)
- scikit-learn (for clustering and similarity analysis)
- numpy (for vector operations)
- matplotlib (for visualization)

```bash
pip install openai tiktoken tqdm scikit-learn numpy matplotlib
```

## Core Components

### Answer Extraction

```bash
python extract_answers.py --api_key YOUR_OPENAI_API_KEY --input_pattern "responses/*.json" --chars 500
```

This script extracts final answers from lengthy LLM-generated solutions by:
- Reading JSON files containing multiple solution attempts
- Extracting the last 500 characters from each solution's text
- Using GPT-4o to identify the final numerical answer
- Saving the extracted answers in a structured format

### Answer Validation

```bash
python compare_answers.py --csv AIME_2023_4.csv --extracted extracted_answers --output comparison_report.md
```

This script validates the extracted answers against ground truth by:
- Comparing extracted answers with correct solutions from a reference CSV
- Generating a detailed report of accuracy metrics
- Identifying patterns in correct and incorrect solutions

### Semantic Analysis via Chunking and Embedding

```bash
python chunk_and_embed.py --api_key YOUR_OPENAI_API_KEY --input_pattern "responses/*.json" --output_dir chunked_embeddings --chunk_size 300
```

This script prepares reasoning chains for semantic analysis by:
- Splitting each solution into 300-token chunks
- Creating vector embeddings for each chunk using OpenAI's embedding models
- Organizing chunks and embeddings for further clustering analysis

### Clustering-based Pruning

```bash
python cluster_embeddings.py --csv AIME_2023_4.csv --extracted extracted_answers --embeddings chunked_embeddings --cluster_sizes "10,15,20,25,30,35,40,45" --midway_only
```

This script implements our novel clustering-based pruning approach by:
- Applying k-means clustering to solution embeddings
- Evaluating various cluster sizes to determine optimal pruning parameters
- Assessing whether correct answers are preserved in the pruned solution set
- Generating a comprehensive report on pruning effectiveness (ie do the pruned reasoning chains still contain a correct answer to the given question)

### Baseline Analysis

```bash
python calculate_pass_at_k.py --csv AIME_2023_4.csv --extracted extracted_answers --n 50  --output pass_at_k.png
```

This script shows what the estimated baseline performance of the model when given 1->n attempts. This serves as a baseline that this experiment hopes to improve upon.


## Research Findings

Our research demonstrates that effective pruning of parallel reasoning chains can significantly improve computational efficiency while maintaining solution accuracy. The clustering-based approach we've developed shows particular promise for:

1. Identifying and preserving correct reasoning paths
2. Reducing the computational resources needed for large-scale problem solving
3. Enabling more efficient scaling of LLM-based reasoning on verifiable domains

The graphs that support this conclusion are original_custom_pass_at_k.png and val_custom_pass_at_k.png, which highlight how this method outperforms the baseline pass @ 10 and matches the perfomance of pass @ 50 by pruning 40 reasoning chains based off clustering done at the first 300 tokens of output, massively reducing computational cost. 

## Limitations
So far this work was done with a limited set of benchmarks, mainly focusing on the AIME 2023->2024. In order to fully conclude the effectiveness of this result, more extensive benchmarking will need to be done in other verifiable domains such as code, chess puzzles, proof writing, etc. This work also only implements embedding-based methods for determining when to prune reasoning chains, where other approaches, such as using an Auto-Regressive LM to judge the most diverse chains of thought, as well as using other traditional metrics for measuring semantic similarity in NLP (BLEU, ROUGE, BLUERT), may perform equally well and even generalize better. This work is also a precursor to an efficient implementation on both GPUS and custom ASICs.

## Usage Example

A typical research workflow using these scripts would be:

1. Use process_aime_responses.py to collect the reasoning chains from a model of your choice
2. Extract answers from JSON files using `extract_answers.py`
3. Validate extracted answers against ground truth using `compare_answers.py`
4. Chunk and embed solutions using `chunk_and_embed.py`
5. Apply clustering-based pruning and performance metrics `cluster_embeddings.py`
6. Analyze baseline performance using `calculate_pass_at_k.py`
